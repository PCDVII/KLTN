{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ab71275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "alt.data_transformers.enable('default', max_rows=None)\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append(\"..\") # to import from parent directory\n",
    "from dataset import CSIDataset\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import logging\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af0ad4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AnalyseLSTMClassifier(nn.Module):\n",
    "    \"\"\"Very simple implementation of LSTM-based time-series classifier.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, bidirectional, batch_size):\n",
    "        super(AnalyseLSTMClassifier, self).__init__()\n",
    "        self.arch = \"lstm\"\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_dir = 2 if bidirectional else 1\n",
    "\n",
    "        self.ILayer = nn.LSTMCell(input_dim, hidden_dim)\n",
    "        self.layer1 = nn.LSTMCell(hidden_dim, hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim*self.num_dir, hidden_dim)\n",
    "        self.layer3 = nn.ReLU(True)\n",
    "        self.layer4 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer5 = nn.ReLU(True)\n",
    "        self.OLayer = nn.Linear(hidden_dim, output_dim)\n",
    "        self.hidden = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len, batch_size, _ = x.size()\n",
    "        if self.hidden is None:\n",
    "            self.hidden = self.init_hidden(batch_size)\n",
    "            h0, c0, h1, c1 = self.hidden\n",
    "        else:\n",
    "            h0, c0, h1, c1 = self.hidden\n",
    "        outputIL, outputL1 = [], []\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            h0, c0 = self.ILayer(x[t], (h0, c0))\n",
    "            outputIL.append(h0)\n",
    "            h1, c1 = self.layer1(h0, (h1, c1))\n",
    "            outputL1.append(h1)\n",
    "        outputIL = torch.stack(outputIL, dim=0)\n",
    "        outputL1 = torch.stack(outputL1, dim=0)\n",
    "        outputL2 = self.layer2(outputL1[-1,:,:])\n",
    "        outputL3 = self.layer3(outputL2)\n",
    "        outputL4 = self.layer4(outputL3)\n",
    "        outputL5 = self.layer5(outputL4)\n",
    "        outputL = self.OLayer(outputL5) #class probabilities\n",
    "\n",
    "        return outputL\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h0 = nn.Parameter(nn.init.xavier_uniform_(\n",
    "                torch.Tensor(batch_size, self.hidden_dim).type(torch.DoubleTensor)\n",
    "            ), requires_grad=True).to(device)\n",
    "\n",
    "        c0 = nn.Parameter(nn.init.xavier_uniform_(\n",
    "                torch.Tensor(batch_size, self.hidden_dim).type(torch.DoubleTensor)\n",
    "            ), requires_grad=True).to(device)\n",
    "        h1 = nn.Parameter(nn.init.xavier_uniform_(\n",
    "                torch.Tensor(batch_size, self.hidden_dim).type(torch.DoubleTensor)\n",
    "            ), requires_grad=True).to(device)\n",
    "\n",
    "        c1 = nn.Parameter(nn.init.xavier_uniform_(\n",
    "                torch.Tensor(batch_size, self.hidden_dim).type(torch.DoubleTensor)\n",
    "            ), requires_grad=True).to(device)\n",
    "\n",
    "        return h0, c0, h1, c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6fe56c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_metric_demo(model, dl, criterion, BATCH_SIZE):\n",
    "    model.eval()\n",
    "\n",
    "    correct, total, total_loss = 0, 0, 0\n",
    "\n",
    "    model.hidden = model.init_hidden(BATCH_SIZE)\n",
    "    for x_val, y_val in tqdm(dl, total=len(dl), desc=\"Validation epoch: \"):\n",
    "        if x_val.size(0) != BATCH_SIZE:\n",
    "            continue\n",
    "        model.init_hidden(x_val.size(0))\n",
    "        x_val = x_val.permute(1, 0, 2)\n",
    "        \n",
    "        x_val = x_val.double().to(device)\n",
    "        y_val = y_val.double().to(device)\n",
    "\n",
    "        out = model(x_val)\n",
    "\n",
    "        loss = criterion(out, y_val.long())\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        preds = F.log_softmax(out, dim=1).argmax(dim=1)\n",
    "        total += y_val.size(0)\n",
    "        correct += (preds == y_val).sum().item()\n",
    "\n",
    "    acc = correct / total\n",
    "\n",
    "    return total_loss, correct, total, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "acdac8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Cuda support\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "logging.info(\"Device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36b7c0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 468\n",
    "hidden_dim = 256\n",
    "dropout = 0.0\n",
    "output_dim = 7\n",
    "batch_size = 4\n",
    "model = AnalyseLSTMClassifier(input_dim, hidden_dim, output_dim, False, batch_size).double().to(device)\n",
    "EPOCHS_NUM = 2\n",
    "BATCH_SIZE = 4\n",
    "SEQ_DIM = 1024\n",
    "DATA_STEP = 1\n",
    "\n",
    "class_weights = (\n",
    "    torch.Tensor([0.113, 0.439, 0.0379, 0.1515, 0.0379, 0.1212, 0.1363])\n",
    "    .double()\n",
    "    .to(device)\n",
    ")\n",
    "class_weights_inv = 1 / class_weights\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_inv)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00146)\n",
    "scheduler = ReduceLROnPlateau(optimizer, \"min\", factor=0.5)\n",
    "patience, trials, best_acc = 100, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a37f868c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    logging.info(\"Loading data...\")\n",
    "\n",
    "    train_dataset = CSIDataset(\n",
    "        [\n",
    "            \"D:\\\\Gitdesktop\\\\KLTN\\\\pythonFile\\\\data\\\\room_1\\\\1\",\n",
    "            # \"D:\\\\Gitdesktop\\\\KLTN\\\\pythonFile\\\\data\\\\room_1\\\\2\",\n",
    "            # \"D:\\\\Gitdesktop\\\\KLTN\\\\pythonFile\\\\data\\\\room_1\\\\3\",\n",
    "            # \"D:\\\\Gitdesktop\\\\KLTN\\\\pythonFile\\\\data\\\\room_1\\\\4\",\n",
    "            # \"D:\\\\Gitdesktop\\\\KLTN\\\\pythonFile\\\\data\\\\room_2\\\\1\",\n",
    "            # \"D:\\\\Gitdesktop\\\\KLTN\\\\pythonFile\\\\data\\\\room_2\\\\2\",\n",
    "            # \"D:\\\\Gitdesktop\\\\KLTN\\\\pythonFile\\\\data\\\\room_3\\\\1\",\n",
    "            # \"D:\\\\Gitdesktop\\\\KLTN\\\\pythonFile\\\\data\\\\room_3\\\\2\",\n",
    "            # \"D:\\\\Gitdesktop\\\\KLTN\\\\pythonFile\\\\data\\\\room_3\\\\3\",\n",
    "            # \"D:\\\\Gitdesktop\\\\KLTN\\\\pythonFile\\\\data\\\\room_3\\\\4\",\n",
    "            # \"D:\\\\Gitdesktop\\\\KLTN\\\\pythonFile\\\\data\\\\room_3\\\\5\",\n",
    "        ],\n",
    "        SEQ_DIM,\n",
    "        DATA_STEP,\n",
    "    )\n",
    "\n",
    "    val_dataset = train_dataset\n",
    "\n",
    "    logging.info(\"Data is loaded...\")\n",
    "\n",
    "    trn_dl = DataLoader(\n",
    "        train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0\n",
    "    )\n",
    "    val_dl = DataLoader(\n",
    "        val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0\n",
    "    )\n",
    "\n",
    "    return trn_dl, val_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95db626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading data...\n",
      "INFO:root:Data is loaded...\n",
      "INFO:root:Start model training\n",
      "Training epoch:   0%|          | 0/1052 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 4, 468])\n",
      "Output:  tensor([[ 0.0087, -0.0068, -0.0407, -0.0395, -0.0315,  0.0582,  0.0368],\n",
      "        [ 0.0077, -0.0059, -0.0436, -0.0413, -0.0310,  0.0573,  0.0349],\n",
      "        [ 0.0081, -0.0062, -0.0425, -0.0420, -0.0321,  0.0580,  0.0353],\n",
      "        [ 0.0082, -0.0064, -0.0417, -0.0419, -0.0302,  0.0577,  0.0354]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "Predict:  tensor([5, 5, 5, 5])\n",
      "y_batch:  tensor([5, 1, 1, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch:   0%|          | 0/1052 [00:04<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "trn_dl, val_dl = load_data()\n",
    "logging.info(\"Start model training\")\n",
    "for epoch in range(1, EPOCHS_NUM + 1):\n",
    "    model.train(mode=True)\n",
    "    for i, (x_batch, y_batch) in tqdm(enumerate(trn_dl), total=len(trn_dl), desc=\"Training epoch: \"):\n",
    "        if x_batch.size(0) != 4:\n",
    "            continue\n",
    "        x_batch = x_batch.permute(1, 0, 2).double().to(device) # (seq_len, batch, input_size) \n",
    "        print(x_batch.shape)\n",
    "        print(\"x_batch at seq 0: \",x_batch[0]) #input data\n",
    "        print(\"x_batch at seq 1: \",x_batch[1])\n",
    "        output = model(x_batch)\n",
    "        print(\"Output: \",output) #output probabilities of next 4 packets \n",
    "        print(\"Predict: \",torch.argmax(torch.nn.functional.log_softmax(output, dim=1), dim=1))  # predicted class\n",
    "        print(\"y_batch: \",y_batch) #actual class  \n",
    "        loss = criterion(output, y_batch.long())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i == 1:\n",
    "            break\n",
    "    break\n",
    "    val_loss, val_correct, val_total, val_acc = get_train_metric_demo(\n",
    "            model, val_dl, criterion, batch_size\n",
    "    )\n",
    "    train_loss, train_correct, train_total, train_acc = get_train_metric_demo(\n",
    "        model, trn_dl, criterion, batch_size\n",
    "    )\n",
    "\n",
    "    logging.info(\n",
    "    f\"Epoch: {epoch:3d} |\"\n",
    "    f\" Validation Loss: {val_loss:.2f}, Validation Acc.: {val_acc:2.2%}, \"\n",
    "    f\"Train Loss: {train_loss:.2f}, Train Acc.: {train_acc:2.2%}\"\n",
    "    )\n",
    "    if val_acc > best_acc:\n",
    "        trials = 0\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"D:\\\\Gitdesktop\\\\WIFI_CSI_based_HAR\\\\model\\\\saveModels\\\\lstm_best.pth\")\n",
    "        logging.info(\n",
    "            f\"Epoch {epoch} best model saved with accuracy: {best_acc:2.2%}\"\n",
    "        )\n",
    "    else:\n",
    "        trials += 1\n",
    "        if trials >= patience:\n",
    "            logging.info(f\"Early stopping on epoch {epoch}\")\n",
    "            break\n",
    "    scheduler.step(val_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
